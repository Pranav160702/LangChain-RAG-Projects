{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Langchain Components to use\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Support for dataset retrieval with Huggingface \n",
    "from datasets import load_dataset\n",
    "\n",
    "# With CassIO, the engine powering the Astra DB integration in Langchain.\n",
    "# Will also initialize the DB Connection\n",
    "import cassio\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith Tracing SetUp\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'Langchain - Text To Math Problem Solver and Data Search Assistant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASTRA_DB_APPLICATION_TOKEN = \"enter-your-token\"\n",
    "ASTRA_DB_ID = \"enter-your-id\"\n",
    "\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the Dcument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader = PdfReader('LLM.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "\n",
    "# Read text from PDF\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize The Connection to DataBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(token = ASTRA_DB_APPLICATION_TOKEN, database_id = ASTRA_DB_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Langchain Embedding And LLM objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Langchain Projects\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(model = 'llama3-8b-8192', groq_api_key = GROQ_API_KEY)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Langchain VectorStore... Backed by Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_vector_store = Cassandra(\n",
    "    embedding = embedding,\n",
    "    table_name = \"qa_mini_demo\",\n",
    "    session = None, #Default Parameter\n",
    "    keyspace = None #Default Parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the Dataset into the VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 119 headlines.\n"
     ]
    }
   ],
   "source": [
    "astra_vector_store.add_texts(texts)\n",
    "\n",
    "print('Inserted %i headlines.'% len(texts))\n",
    "\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore = astra_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the QA Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_question = True\n",
    "while True:\n",
    "    if first_question:\n",
    "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
    "    else:\n",
    "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
    "\n",
    "    if query_text.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    if query_text == \"\":\n",
    "        continue\n",
    "\n",
    "    first_question = False\n",
    "\n",
    "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
    "\n",
    "    # Quering the vectorstore DB\n",
    "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
    "\n",
    "\n",
    "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
    "\n",
    "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
    "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
    "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
